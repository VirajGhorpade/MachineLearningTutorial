{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisrt Neural Network In Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisrt MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PimaIndians Dataset\n",
    "\n",
    "\n",
    "Predict the Onset of Diabetes\n",
    "\n",
    "Data mining and machine learning is helping medical professionals make diagnosis easier by bridging the gap between huge data sets and human knowledge. We can begin to apply machine learning techniques for classification in a dataset that describes a population that is under a high risk of the onset of diabetes.\n",
    "\n",
    "Diabetes Mellitus affects 382 million people in the world, and the number of people with type-2 diabetes is increasing in every country. Untreated, diabetes can cause many complications.\n",
    "\n",
    "Diabetes\n",
    "Diabetes Test\n",
    "Photo by Victor, some rights reserved.\n",
    "The population for this study was the Pima Indian population near Phoenix, Arizona. The population has been under continuous study since 1965 by the National Institute of Diabetes and Digestive and Kidney Diseases because of its high incidence rate of diabetes.\n",
    "\n",
    "For the purposes of this dataset, diabetes was diagnosed according to World Health Organization Criteria, which stated that if the 2 hour post-load glucose was at least 200 mg/dl at any survey exam or if the Indian Health Service Hospital serving the community found a glucose concentration of at least 200 mg/dl during the course of routine medical care.\n",
    "\n",
    "Given the medical data we can gather about people, we should be able to make better predictions on how likely a person is to suffer the onset of diabetes, and therefore act appropriately to help. We can start analyzing data and experimenting with algorithms that will help us study the onset of diabetes in Pima Indians.\n",
    "\n",
    "Related Work\n",
    "\n",
    "Our study begins with an in-depth look of how researchers that used the same dataset approached the same problem. This helped me gain an understanding of the data and pave the way for my study, especially since the authors suggested alternative methods that are worth researching.\n",
    "\n",
    "In 1988, Smith, Everhart, Dickson, Knowler, and Johannes performed an evaluation of using an early neural network model, ADAP, to forecast the onset of diabetes mellitus in a high-risk population of Pima Indians. They argued that the neural network approach would provide strong results when “the sample size is small, the form of underlying functional relationship is not known, and the underlying functional relationships involve complex interactions and intercorrelations among a number of variables“, see Using the ADAP Learning Algorithm to Forecast the Onset of Diabetes Mellitus.\n",
    "\n",
    "They describe ADAP as “an adaptive learning routine that generates and executes digital analogs of perceptron-like devices“, see Learning in Control and Pattern Recognition Systems. The algorithm will make predictions based on a function of input variables, and will make internal adjustments if predictions are incorrect. The network is split into 3 main layers:\n",
    "\n",
    "Input, partitioned into “sensors”: Represents a discrete value. These are organized into partitions and are “excited” by input.\n",
    "Association Units: Uses a threshold function to activate a specific responder value. Connected to adjustable weights that change based on said function.\n",
    "Responder: Responder values are summed and constitute a specific prediction.\n",
    "The network defined a “fixed matrix” that contained a partition for each attribute, a range of possible values, and the ability to identify connections in the data through a “variable array.” Rows in the matrix correspond to sensors, while columns correspond to association units. The variable array provided a way to easily identify connections between sensors and association units.\n",
    "\n",
    "768 Females of Pima Indian Heritage\n",
    "\n",
    "We can learn from the data found on UCI Machine Learning Repository which contains data on female patients at least 21 years old of Pima Indian heritage.\n",
    "\n",
    "We have 768 instances and the following 8 attributes:\n",
    "\n",
    "1. Number of times pregnant.\n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
    "3. Diastolic blood pressure (mm Hg).\n",
    "4. Triceps skin fold thickness (mm).\n",
    "5. 2-Hour serum insulin (mu U/ml).\n",
    "6. Body mass index.\n",
    "7. Diabetes pedigree function.\n",
    "8. Age (years).\n",
    "\n",
    "9. Class, onset of diabetes within five years.  (Output)\n",
    "\n",
    "\n",
    "\n",
    "A particularly interesting attribute used in the study was the Diabetes Pedigree Function, pedi. It provided some data on diabetes mellitus history in relatives and the genetic relationship of those relatives to the patient. This measure of genetic influence gave us an idea of the hereditary risk one might have with the onset of diabetes mellitus. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Create your first MLP in Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221.0\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "768/768 [==============================] - 0s - loss: 3.7327 - acc: 0.6016     \n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s - loss: 0.9372 - acc: 0.5911     \n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s - loss: 0.7479 - acc: 0.6458     \n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s - loss: 0.7116 - acc: 0.6589     \n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s - loss: 0.6810 - acc: 0.6732     \n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s - loss: 0.6507 - acc: 0.6810     \n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s - loss: 0.6493 - acc: 0.6693     \n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s - loss: 0.6368 - acc: 0.6901     \n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s - loss: 0.6241 - acc: 0.6901     \n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s - loss: 0.6299 - acc: 0.6797     \n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s - loss: 0.6479 - acc: 0.6745     \n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s - loss: 0.6391 - acc: 0.6745     \n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s - loss: 0.6254 - acc: 0.6771     \n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s - loss: 0.6185 - acc: 0.7005     \n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s - loss: 0.6023 - acc: 0.6979     \n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s - loss: 0.5875 - acc: 0.7018     \n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s - loss: 0.5850 - acc: 0.6992     \n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s - loss: 0.5988 - acc: 0.6862     \n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s - loss: 0.5803 - acc: 0.7096     \n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s - loss: 0.5792 - acc: 0.7201     \n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s - loss: 0.5703 - acc: 0.7135     \n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s - loss: 0.5823 - acc: 0.6979     \n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s - loss: 0.5750 - acc: 0.7096     \n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s - loss: 0.5687 - acc: 0.7292     \n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 0s - loss: 0.5584 - acc: 0.7383     \n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 0s - loss: 0.5703 - acc: 0.7083     \n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 0s - loss: 0.5557 - acc: 0.7253     \n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 0s - loss: 0.5551 - acc: 0.7292     \n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 0s - loss: 0.5727 - acc: 0.7161     \n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 0s - loss: 0.5610 - acc: 0.7214     \n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 0s - loss: 0.5686 - acc: 0.7161     \n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 0s - loss: 0.5641 - acc: 0.7135     \n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 0s - loss: 0.5515 - acc: 0.7240     \n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s - loss: 0.5479 - acc: 0.7318     \n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s - loss: 0.5493 - acc: 0.7188     \n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s - loss: 0.5645 - acc: 0.7057     \n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s - loss: 0.5334 - acc: 0.7344     \n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s - loss: 0.5400 - acc: 0.7279     \n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s - loss: 0.5463 - acc: 0.7240     \n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s - loss: 0.5454 - acc: 0.7253     \n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s - loss: 0.5436 - acc: 0.7292     \n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s - loss: 0.5387 - acc: 0.7448     \n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 0s - loss: 0.5315 - acc: 0.7539     \n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s - loss: 0.5330 - acc: 0.7422     \n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s - loss: 0.5321 - acc: 0.7448     \n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 0s - loss: 0.5309 - acc: 0.7513     \n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s - loss: 0.5311 - acc: 0.7396     \n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s - loss: 0.5322 - acc: 0.7409     \n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s - loss: 0.5331 - acc: 0.7500     \n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s - loss: 0.5267 - acc: 0.7344     \n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s - loss: 0.5282 - acc: 0.7461     \n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s - loss: 0.5308 - acc: 0.7448     \n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s - loss: 0.5385 - acc: 0.7409     \n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s - loss: 0.5377 - acc: 0.7305     \n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s - loss: 0.5221 - acc: 0.7526     \n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s - loss: 0.5287 - acc: 0.7396     \n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s - loss: 0.5310 - acc: 0.7331     \n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s - loss: 0.5226 - acc: 0.7539     \n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s - loss: 0.5119 - acc: 0.7617     \n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 0s - loss: 0.5340 - acc: 0.7409     \n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s - loss: 0.5266 - acc: 0.7383     \n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s - loss: 0.5169 - acc: 0.7552     \n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s - loss: 0.5433 - acc: 0.7357     \n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 0s - loss: 0.5315 - acc: 0.7409     \n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 0s - loss: 0.5198 - acc: 0.7461     \n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 0s - loss: 0.5062 - acc: 0.7526     \n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s - loss: 0.5159 - acc: 0.7383     \n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 0s - loss: 0.5129 - acc: 0.7526     \n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 0s - loss: 0.5121 - acc: 0.7513     \n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 0s - loss: 0.5368 - acc: 0.7214     \n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s - loss: 0.5168 - acc: 0.7409     \n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s - loss: 0.5168 - acc: 0.7500     \n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 0s - loss: 0.5162 - acc: 0.7500     \n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 0s - loss: 0.5104 - acc: 0.7578     \n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 0s - loss: 0.5090 - acc: 0.7578     \n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 0s - loss: 0.5112 - acc: 0.7578     \n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 0s - loss: 0.5154 - acc: 0.7604     \n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s - loss: 0.5121 - acc: 0.7500     \n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s - loss: 0.5127 - acc: 0.7474     \n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s - loss: 0.5107 - acc: 0.7565     \n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 0s - loss: 0.5052 - acc: 0.7682     \n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s - loss: 0.5052 - acc: 0.7539     \n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 0s - loss: 0.4988 - acc: 0.7604     \n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s - loss: 0.4983 - acc: 0.7578     \n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s - loss: 0.5051 - acc: 0.7474     \n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s - loss: 0.5043 - acc: 0.7448     \n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s - loss: 0.4986 - acc: 0.7539     \n",
      "Epoch 88/150\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.4889 - acc: 0.781 - 0s - loss: 0.5011 - acc: 0.7630     \n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 0s - loss: 0.5056 - acc: 0.7708     \n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s - loss: 0.5074 - acc: 0.7552     \n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s - loss: 0.5027 - acc: 0.7539     \n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s - loss: 0.5044 - acc: 0.7435     \n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s - loss: 0.4976 - acc: 0.7682     \n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s - loss: 0.4985 - acc: 0.7669     \n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s - loss: 0.5033 - acc: 0.7474     \n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s - loss: 0.4910 - acc: 0.7695     \n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s - loss: 0.4966 - acc: 0.7786     \n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s - loss: 0.4885 - acc: 0.7630     \n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s - loss: 0.4890 - acc: 0.7656     \n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s - loss: 0.4839 - acc: 0.7747     \n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s - loss: 0.4870 - acc: 0.7773     \n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s - loss: 0.4982 - acc: 0.7591     \n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 0s - loss: 0.4979 - acc: 0.7591     \n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s - loss: 0.4891 - acc: 0.7865     \n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s - loss: 0.5256 - acc: 0.7487     \n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s - loss: 0.4915 - acc: 0.7747     \n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s - loss: 0.4896 - acc: 0.7760     \n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s - loss: 0.4964 - acc: 0.7760     \n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s - loss: 0.4860 - acc: 0.7708     \n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s - loss: 0.4906 - acc: 0.7721     \n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s - loss: 0.4829 - acc: 0.7865     \n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s - loss: 0.4923 - acc: 0.7773     \n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s - loss: 0.4903 - acc: 0.7578     \n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s - loss: 0.4916 - acc: 0.7591     \n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s - loss: 0.4903 - acc: 0.7734     \n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s - loss: 0.4937 - acc: 0.7643     \n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s - loss: 0.4903 - acc: 0.7656     \n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s - loss: 0.4883 - acc: 0.7826     \n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s - loss: 0.4822 - acc: 0.7734     \n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s - loss: 0.4928 - acc: 0.7786     \n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s - loss: 0.4919 - acc: 0.7708     \n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 0s - loss: 0.4863 - acc: 0.7799     \n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s - loss: 0.4805 - acc: 0.7695     \n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s - loss: 0.4830 - acc: 0.7747     \n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s - loss: 0.4875 - acc: 0.7852     \n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s - loss: 0.4800 - acc: 0.7839     \n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s - loss: 0.4908 - acc: 0.7695     \n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s - loss: 0.4725 - acc: 0.7760     \n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s - loss: 0.4818 - acc: 0.7695     \n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s - loss: 0.4754 - acc: 0.7865     \n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 0s - loss: 0.4819 - acc: 0.7734     \n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s - loss: 0.4831 - acc: 0.7826     \n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s - loss: 0.4830 - acc: 0.7669     \n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s - loss: 0.4837 - acc: 0.7760     \n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s - loss: 0.4784 - acc: 0.7760     \n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s - loss: 0.4752 - acc: 0.7786     \n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s - loss: 0.4680 - acc: 0.7813     \n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s - loss: 0.4799 - acc: 0.7826     \n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s - loss: 0.4657 - acc: 0.7891     \n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s - loss: 0.4831 - acc: 0.7786     \n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s - loss: 0.4723 - acc: 0.7812     \n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s - loss: 0.4807 - acc: 0.7669     \n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s - loss: 0.4763 - acc: 0.7773     \n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s - loss: 0.4759 - acc: 0.7760     \n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s - loss: 0.4870 - acc: 0.7695     \n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s - loss: 0.4984 - acc: 0.7669     \n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 0s - loss: 0.4840 - acc: 0.7839     \n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 0s - loss: 0.4726 - acc: 0.7708     \n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s - loss: 0.4740 - acc: 0.7643     \n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s - loss: 0.4748 - acc: 0.7682     \n",
      " 32/768 [>.............................] - ETA: 0s\n",
      "acc: 78.52%\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X, Y, epochs=150, batch_size=10)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate The Performance Of Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 2/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 3/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 4/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 5/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 6/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 7/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 8/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 9/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 10/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 11/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 12/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 13/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 14/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 15/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 16/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 17/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 18/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 19/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 20/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 21/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 22/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 23/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 24/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 25/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 26/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 27/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 28/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 29/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 30/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 31/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 32/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 33/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 34/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 35/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 36/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 37/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 38/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 39/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 40/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 41/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 42/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 43/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 44/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 45/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 46/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 47/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 48/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 49/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 50/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 51/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 52/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 53/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 54/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 55/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 56/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 57/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 58/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 59/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 60/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 61/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 62/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 63/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 64/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 65/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 66/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 67/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 68/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 69/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 70/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 71/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 72/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 73/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 74/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 75/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 76/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 77/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 78/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 79/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 80/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 81/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 82/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 83/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 84/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 85/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 86/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 87/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 88/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 89/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 90/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 91/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 92/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 93/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 94/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 95/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 96/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 97/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 98/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 99/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 100/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 101/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 102/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 103/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 104/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 105/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 106/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 107/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 108/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 109/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 110/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 111/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 112/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 113/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 114/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 115/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 116/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 117/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 118/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 119/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 120/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 121/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 122/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 123/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 124/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 125/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 126/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 127/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 128/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 129/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 130/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 131/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 132/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 133/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 134/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 135/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 136/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 137/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 138/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 139/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 140/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 141/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 142/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 143/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 144/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 145/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 146/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 147/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 148/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 149/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 150/150\n",
      "514/514 [==============================] - 0s - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c628932dd8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP with automatic validation set\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 2/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 3/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 4/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 5/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 6/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 7/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 8/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 9/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 10/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 11/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 12/150\n",
      "514/514 [==============================] - ETA: 0s - loss: 5.4443 - acc: 0.662 - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 13/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 14/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 15/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 16/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 17/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 18/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 19/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 20/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 21/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 22/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 23/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 24/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 25/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 26/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 27/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 28/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 29/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 30/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 31/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 32/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 33/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 34/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 35/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 36/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 37/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 38/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 39/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 40/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 41/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 42/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 43/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 44/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 45/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 46/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 47/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 48/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 49/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 50/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 51/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 52/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 53/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 54/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 55/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 56/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 57/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 58/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 59/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 60/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 61/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 62/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 63/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 64/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 65/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 66/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 67/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 68/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 69/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 70/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 71/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 72/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 73/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 74/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 75/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 76/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 77/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 78/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 79/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 80/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 81/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 82/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 83/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 84/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 85/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 86/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 87/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 88/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 89/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 90/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 91/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 92/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 93/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 94/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 95/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 96/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 97/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 98/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 99/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 100/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 101/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 102/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 103/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 104/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 105/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 106/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 107/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 108/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 109/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 110/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 111/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 112/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 113/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 114/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 115/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 116/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 117/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 118/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 119/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 120/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 121/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 122/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 123/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 124/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 125/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 126/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 127/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 128/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 129/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 130/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 131/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 132/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 133/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 134/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 135/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 136/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 137/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 138/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 139/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 140/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 141/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 142/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 143/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 144/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 145/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 146/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 147/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 148/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 149/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 150/150\n",
      "514/514 [==============================] - 0s - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c62a9fdba8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP with manual validation set\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# split into 67% for train and 33% for test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MLP for Pima Indians Dataset with 10-fold cross validation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221.0\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "acc: 80.52%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221.0\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "acc: 75.32%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221.0\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "acc: 71.43%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221.0\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "acc: 72.73%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221.0\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "acc: 72.73%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221.0\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "acc: 64.94%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221.0\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-7b08573f43c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;31m# evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\DeepLearning\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1140\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1141\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\DeepLearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2073\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2074\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define 10-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, Y):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    #model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # Fit the model\n",
    "    model.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Keras with Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(8, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "inits = ['glorot_uniform', 'normal']\n",
    "epochs = [50, 100]\n",
    "batches = [5, 10]\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save models with Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialize the Model to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221.0\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "acc: 76.43%\n"
     ]
    }
   ],
   "source": [
    "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "import numpy\n",
    "import os\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "model.summary()\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Loaded model from disk\n",
      "acc: 76.43%\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# later...\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep The Best Models During Training Using Model Check Pointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Point Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_acc improved from -inf to 0.67323, saving model to weights.best.hdf5\n",
      "Epoch 00001: val_acc did not improve\n",
      "Epoch 00002: val_acc did not improve\n",
      "Epoch 00003: val_acc did not improve\n",
      "Epoch 00004: val_acc improved from 0.67323 to 0.67717, saving model to weights.best.hdf5\n",
      "Epoch 00005: val_acc improved from 0.67717 to 0.69291, saving model to weights.best.hdf5\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 00007: val_acc improved from 0.69291 to 0.70866, saving model to weights.best.hdf5\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 00011: val_acc improved from 0.70866 to 0.70866, saving model to weights.best.hdf5\n",
      "Epoch 00012: val_acc did not improve\n",
      "Epoch 00013: val_acc did not improve\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 00015: val_acc did not improve\n",
      "Epoch 00016: val_acc did not improve\n",
      "Epoch 00017: val_acc improved from 0.70866 to 0.71260, saving model to weights.best.hdf5\n",
      "Epoch 00018: val_acc did not improve\n",
      "Epoch 00019: val_acc did not improve\n",
      "Epoch 00020: val_acc did not improve\n",
      "Epoch 00021: val_acc did not improve\n",
      "Epoch 00022: val_acc did not improve\n",
      "Epoch 00023: val_acc did not improve\n",
      "Epoch 00024: val_acc did not improve\n",
      "Epoch 00025: val_acc did not improve\n",
      "Epoch 00026: val_acc did not improve\n",
      "Epoch 00027: val_acc did not improve\n",
      "Epoch 00028: val_acc did not improve\n",
      "Epoch 00029: val_acc did not improve\n",
      "Epoch 00030: val_acc did not improve\n",
      "Epoch 00031: val_acc did not improve\n",
      "Epoch 00032: val_acc did not improve\n",
      "Epoch 00033: val_acc did not improve\n",
      "Epoch 00034: val_acc did not improve\n",
      "Epoch 00035: val_acc improved from 0.71260 to 0.71654, saving model to weights.best.hdf5\n",
      "Epoch 00036: val_acc did not improve\n",
      "Epoch 00037: val_acc did not improve\n",
      "Epoch 00038: val_acc improved from 0.71654 to 0.72835, saving model to weights.best.hdf5\n",
      "Epoch 00039: val_acc did not improve\n",
      "Epoch 00040: val_acc did not improve\n",
      "Epoch 00041: val_acc did not improve\n",
      "Epoch 00042: val_acc did not improve\n",
      "Epoch 00043: val_acc improved from 0.72835 to 0.73622, saving model to weights.best.hdf5\n",
      "Epoch 00044: val_acc did not improve\n",
      "Epoch 00045: val_acc did not improve\n",
      "Epoch 00046: val_acc improved from 0.73622 to 0.73622, saving model to weights.best.hdf5\n",
      "Epoch 00047: val_acc improved from 0.73622 to 0.74016, saving model to weights.best.hdf5\n",
      "Epoch 00048: val_acc did not improve\n",
      "Epoch 00049: val_acc did not improve\n",
      "Epoch 00050: val_acc did not improve\n",
      "Epoch 00051: val_acc did not improve\n",
      "Epoch 00052: val_acc did not improve\n",
      "Epoch 00053: val_acc improved from 0.74016 to 0.74409, saving model to weights.best.hdf5\n",
      "Epoch 00054: val_acc did not improve\n",
      "Epoch 00055: val_acc did not improve\n",
      "Epoch 00056: val_acc did not improve\n",
      "Epoch 00057: val_acc did not improve\n",
      "Epoch 00058: val_acc did not improve\n",
      "Epoch 00059: val_acc did not improve\n",
      "Epoch 00060: val_acc did not improve\n",
      "Epoch 00061: val_acc did not improve\n",
      "Epoch 00062: val_acc did not improve\n",
      "Epoch 00063: val_acc improved from 0.74409 to 0.75591, saving model to weights.best.hdf5\n",
      "Epoch 00064: val_acc did not improve\n",
      "Epoch 00065: val_acc improved from 0.75591 to 0.76772, saving model to weights.best.hdf5\n",
      "Epoch 00066: val_acc did not improve\n",
      "Epoch 00067: val_acc did not improve\n",
      "Epoch 00068: val_acc did not improve\n",
      "Epoch 00069: val_acc did not improve\n",
      "Epoch 00070: val_acc did not improve\n",
      "Epoch 00071: val_acc improved from 0.76772 to 0.76772, saving model to weights.best.hdf5\n",
      "Epoch 00072: val_acc did not improve\n",
      "Epoch 00073: val_acc did not improve\n",
      "Epoch 00074: val_acc did not improve\n",
      "Epoch 00075: val_acc did not improve\n",
      "Epoch 00076: val_acc did not improve\n",
      "Epoch 00077: val_acc did not improve\n",
      "Epoch 00078: val_acc did not improve\n",
      "Epoch 00079: val_acc did not improve\n",
      "Epoch 00080: val_acc did not improve\n",
      "Epoch 00081: val_acc improved from 0.76772 to 0.77165, saving model to weights.best.hdf5\n",
      "Epoch 00082: val_acc did not improve\n",
      "Epoch 00083: val_acc did not improve\n",
      "Epoch 00084: val_acc did not improve\n",
      "Epoch 00085: val_acc improved from 0.77165 to 0.77559, saving model to weights.best.hdf5\n",
      "Epoch 00086: val_acc did not improve\n",
      "Epoch 00087: val_acc did not improve\n",
      "Epoch 00088: val_acc did not improve\n",
      "Epoch 00089: val_acc did not improve\n",
      "Epoch 00090: val_acc did not improve\n",
      "Epoch 00091: val_acc improved from 0.77559 to 0.77953, saving model to weights.best.hdf5\n",
      "Epoch 00092: val_acc did not improve\n",
      "Epoch 00093: val_acc did not improve\n",
      "Epoch 00094: val_acc did not improve\n",
      "Epoch 00095: val_acc did not improve\n",
      "Epoch 00096: val_acc did not improve\n",
      "Epoch 00097: val_acc did not improve\n",
      "Epoch 00098: val_acc did not improve\n",
      "Epoch 00099: val_acc improved from 0.77953 to 0.78346, saving model to weights.best.hdf5\n",
      "Epoch 00100: val_acc did not improve\n",
      "Epoch 00101: val_acc improved from 0.78346 to 0.79134, saving model to weights.best.hdf5\n",
      "Epoch 00102: val_acc did not improve\n",
      "Epoch 00103: val_acc did not improve\n",
      "Epoch 00104: val_acc did not improve\n",
      "Epoch 00105: val_acc did not improve\n",
      "Epoch 00106: val_acc did not improve\n",
      "Epoch 00107: val_acc did not improve\n",
      "Epoch 00108: val_acc did not improve\n",
      "Epoch 00109: val_acc did not improve\n",
      "Epoch 00110: val_acc did not improve\n",
      "Epoch 00111: val_acc did not improve\n",
      "Epoch 00112: val_acc did not improve\n",
      "Epoch 00113: val_acc did not improve\n",
      "Epoch 00114: val_acc did not improve\n",
      "Epoch 00115: val_acc did not improve\n",
      "Epoch 00116: val_acc did not improve\n",
      "Epoch 00117: val_acc improved from 0.79134 to 0.79528, saving model to weights.best.hdf5\n",
      "Epoch 00118: val_acc did not improve\n",
      "Epoch 00119: val_acc did not improve\n",
      "Epoch 00120: val_acc did not improve\n",
      "Epoch 00121: val_acc did not improve\n",
      "Epoch 00122: val_acc did not improve\n",
      "Epoch 00123: val_acc did not improve\n",
      "Epoch 00124: val_acc did not improve\n",
      "Epoch 00125: val_acc did not improve\n",
      "Epoch 00126: val_acc did not improve\n",
      "Epoch 00127: val_acc did not improve\n",
      "Epoch 00128: val_acc did not improve\n",
      "Epoch 00129: val_acc improved from 0.79528 to 0.79921, saving model to weights.best.hdf5\n",
      "Epoch 00130: val_acc did not improve\n",
      "Epoch 00131: val_acc did not improve\n",
      "Epoch 00132: val_acc did not improve\n",
      "Epoch 00133: val_acc did not improve\n",
      "Epoch 00134: val_acc did not improve\n",
      "Epoch 00135: val_acc did not improve\n",
      "Epoch 00136: val_acc did not improve\n",
      "Epoch 00137: val_acc did not improve\n",
      "Epoch 00138: val_acc did not improve\n",
      "Epoch 00139: val_acc did not improve\n",
      "Epoch 00140: val_acc did not improve\n",
      "Epoch 00141: val_acc did not improve\n",
      "Epoch 00142: val_acc did not improve\n",
      "Epoch 00143: val_acc did not improve\n",
      "Epoch 00144: val_acc did not improve\n",
      "Epoch 00145: val_acc did not improve\n",
      "Epoch 00146: val_acc did not improve\n",
      "Epoch 00147: val_acc did not improve\n",
      "Epoch 00148: val_acc did not improve\n",
      "Epoch 00149: val_acc did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c6337c1c88>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checkpoint the weights for best model on validation accuracy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# checkpoint\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Point Model Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_acc improved from -inf to 0.67323, saving model to weights-improvement-00-0.67.hdf5\n",
      "Epoch 00001: val_acc did not improve\n",
      "Epoch 00002: val_acc did not improve\n",
      "Epoch 00003: val_acc improved from 0.67323 to 0.69685, saving model to weights-improvement-03-0.70.hdf5\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 00012: val_acc did not improve\n",
      "Epoch 00013: val_acc did not improve\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 00015: val_acc did not improve\n",
      "Epoch 00016: val_acc did not improve\n",
      "Epoch 00017: val_acc did not improve\n",
      "Epoch 00018: val_acc improved from 0.69685 to 0.70079, saving model to weights-improvement-18-0.70.hdf5\n",
      "Epoch 00019: val_acc did not improve\n",
      "Epoch 00020: val_acc did not improve\n",
      "Epoch 00021: val_acc did not improve\n",
      "Epoch 00022: val_acc did not improve\n",
      "Epoch 00023: val_acc did not improve\n",
      "Epoch 00024: val_acc did not improve\n",
      "Epoch 00025: val_acc did not improve\n",
      "Epoch 00026: val_acc did not improve\n",
      "Epoch 00027: val_acc did not improve\n",
      "Epoch 00028: val_acc did not improve\n",
      "Epoch 00029: val_acc did not improve\n",
      "Epoch 00030: val_acc did not improve\n",
      "Epoch 00031: val_acc did not improve\n",
      "Epoch 00032: val_acc did not improve\n",
      "Epoch 00033: val_acc did not improve\n",
      "Epoch 00034: val_acc did not improve\n",
      "Epoch 00035: val_acc did not improve\n",
      "Epoch 00036: val_acc improved from 0.70079 to 0.72835, saving model to weights-improvement-36-0.73.hdf5\n",
      "Epoch 00037: val_acc did not improve\n",
      "Epoch 00038: val_acc did not improve\n",
      "Epoch 00039: val_acc did not improve\n",
      "Epoch 00040: val_acc did not improve\n",
      "Epoch 00041: val_acc did not improve\n",
      "Epoch 00042: val_acc did not improve\n",
      "Epoch 00043: val_acc did not improve\n",
      "Epoch 00044: val_acc did not improve\n",
      "Epoch 00045: val_acc did not improve\n",
      "Epoch 00046: val_acc improved from 0.72835 to 0.73622, saving model to weights-improvement-46-0.74.hdf5\n",
      "Epoch 00047: val_acc did not improve\n",
      "Epoch 00048: val_acc did not improve\n",
      "Epoch 00049: val_acc did not improve\n",
      "Epoch 00050: val_acc did not improve\n",
      "Epoch 00051: val_acc did not improve\n",
      "Epoch 00052: val_acc improved from 0.73622 to 0.74409, saving model to weights-improvement-52-0.74.hdf5\n",
      "Epoch 00053: val_acc did not improve\n",
      "Epoch 00054: val_acc did not improve\n",
      "Epoch 00055: val_acc did not improve\n",
      "Epoch 00056: val_acc did not improve\n",
      "Epoch 00057: val_acc did not improve\n",
      "Epoch 00058: val_acc did not improve\n",
      "Epoch 00059: val_acc did not improve\n",
      "Epoch 00060: val_acc did not improve\n",
      "Epoch 00061: val_acc did not improve\n",
      "Epoch 00062: val_acc did not improve\n",
      "Epoch 00063: val_acc did not improve\n",
      "Epoch 00064: val_acc did not improve\n",
      "Epoch 00065: val_acc did not improve\n",
      "Epoch 00066: val_acc did not improve\n",
      "Epoch 00067: val_acc did not improve\n",
      "Epoch 00068: val_acc did not improve\n",
      "Epoch 00069: val_acc did not improve\n",
      "Epoch 00070: val_acc did not improve\n",
      "Epoch 00071: val_acc improved from 0.74409 to 0.75984, saving model to weights-improvement-71-0.76.hdf5\n",
      "Epoch 00072: val_acc did not improve\n",
      "Epoch 00073: val_acc did not improve\n",
      "Epoch 00074: val_acc did not improve\n",
      "Epoch 00075: val_acc did not improve\n",
      "Epoch 00076: val_acc did not improve\n",
      "Epoch 00077: val_acc did not improve\n",
      "Epoch 00078: val_acc did not improve\n",
      "Epoch 00079: val_acc did not improve\n",
      "Epoch 00080: val_acc did not improve\n",
      "Epoch 00081: val_acc did not improve\n",
      "Epoch 00082: val_acc did not improve\n",
      "Epoch 00083: val_acc did not improve\n",
      "Epoch 00084: val_acc did not improve\n",
      "Epoch 00085: val_acc did not improve\n",
      "Epoch 00086: val_acc did not improve\n",
      "Epoch 00087: val_acc did not improve\n",
      "Epoch 00088: val_acc did not improve\n",
      "Epoch 00089: val_acc did not improve\n",
      "Epoch 00090: val_acc did not improve\n",
      "Epoch 00091: val_acc did not improve\n",
      "Epoch 00092: val_acc did not improve\n",
      "Epoch 00093: val_acc did not improve\n",
      "Epoch 00094: val_acc did not improve\n",
      "Epoch 00095: val_acc did not improve\n",
      "Epoch 00096: val_acc did not improve\n",
      "Epoch 00097: val_acc did not improve\n",
      "Epoch 00098: val_acc did not improve\n",
      "Epoch 00099: val_acc improved from 0.75984 to 0.77165, saving model to weights-improvement-99-0.77.hdf5\n",
      "Epoch 00100: val_acc did not improve\n",
      "Epoch 00101: val_acc did not improve\n",
      "Epoch 00102: val_acc did not improve\n",
      "Epoch 00103: val_acc did not improve\n",
      "Epoch 00104: val_acc did not improve\n",
      "Epoch 00105: val_acc did not improve\n",
      "Epoch 00106: val_acc did not improve\n",
      "Epoch 00107: val_acc did not improve\n",
      "Epoch 00108: val_acc did not improve\n",
      "Epoch 00109: val_acc did not improve\n",
      "Epoch 00110: val_acc did not improve\n",
      "Epoch 00111: val_acc did not improve\n",
      "Epoch 00112: val_acc did not improve\n",
      "Epoch 00113: val_acc did not improve\n",
      "Epoch 00114: val_acc did not improve\n",
      "Epoch 00115: val_acc did not improve\n",
      "Epoch 00116: val_acc did not improve\n",
      "Epoch 00117: val_acc did not improve\n",
      "Epoch 00118: val_acc did not improve\n",
      "Epoch 00119: val_acc did not improve\n",
      "Epoch 00120: val_acc did not improve\n",
      "Epoch 00121: val_acc did not improve\n",
      "Epoch 00122: val_acc did not improve\n",
      "Epoch 00123: val_acc did not improve\n",
      "Epoch 00124: val_acc did not improve\n",
      "Epoch 00125: val_acc did not improve\n",
      "Epoch 00126: val_acc did not improve\n",
      "Epoch 00127: val_acc did not improve\n",
      "Epoch 00128: val_acc did not improve\n",
      "Epoch 00129: val_acc did not improve\n",
      "Epoch 00130: val_acc did not improve\n",
      "Epoch 00131: val_acc did not improve\n",
      "Epoch 00132: val_acc improved from 0.77165 to 0.77165, saving model to weights-improvement-132-0.77.hdf5\n",
      "Epoch 00133: val_acc did not improve\n",
      "Epoch 00134: val_acc improved from 0.77165 to 0.77165, saving model to weights-improvement-134-0.77.hdf5\n",
      "Epoch 00135: val_acc improved from 0.77165 to 0.77953, saving model to weights-improvement-135-0.78.hdf5\n",
      "Epoch 00136: val_acc did not improve\n",
      "Epoch 00137: val_acc did not improve\n",
      "Epoch 00138: val_acc did not improve\n",
      "Epoch 00139: val_acc did not improve\n",
      "Epoch 00140: val_acc did not improve\n",
      "Epoch 00141: val_acc did not improve\n",
      "Epoch 00142: val_acc did not improve\n",
      "Epoch 00143: val_acc did not improve\n",
      "Epoch 00144: val_acc did not improve\n",
      "Epoch 00145: val_acc did not improve\n",
      "Epoch 00146: val_acc improved from 0.77953 to 0.77953, saving model to weights-improvement-146-0.78.hdf5\n",
      "Epoch 00147: val_acc did not improve\n",
      "Epoch 00148: val_acc did not improve\n",
      "Epoch 00149: val_acc did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c633be2e80>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checkpoint the weights when validation accuracy improves\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand Model Behaviour During Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Model History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_acc', 'val_loss', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwzAMY1z8fxlaV9WztR7cAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2c634fedc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXzSRKKWU6hdNJEoppfpFE4lSSql++f9szmlKWFTUiAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2c635165d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training history\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "history = model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, verbose=0)\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
